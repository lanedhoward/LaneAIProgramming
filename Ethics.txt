https://www.moralmachine.net/

I think the self driving cars should protect innocent bystanders before anything else. 
The passengers of the car made the choice to ride in the self-driving car.
If a human driving a car swerved out of the way of a barrier and killed a pedestrian, we'd call that vehicular manslaughter.
The AI should be held to the same standards.

Past that, I don't think the AI should make judgements about the value of different human lives. I prioritize saving the most
innocent bystanders first. If the numbers are equal, I prioritize upholding the rules of law, avoiding harm to people crossing at
proper times, because those people did not put themselves at risk. 
If all factors are the same, I prioritize non-intervention, as the AI should not make judgements about which lives are more valuable.

The results I think are mostly a wash. Most of my values and decision making process was reflected in them: 
moderate focus on saving more lives, zero focus on protecting passengers, large focus on upholding the law, and slight bias towards
avoiding intervention.

Most of the other categories I had pretty middling scores in, and I know I personally didn't take any of those factors (age, social status)
into account, so any bias there is due to the test itself and what people they put in which situation. If there were more questions
with a random distribution of types of people in different situtations, I bet the biases would even themselves out.